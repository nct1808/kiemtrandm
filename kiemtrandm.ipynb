{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "07649628",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07649628",
        "outputId": "b27bfb90-94a5-43f3-889e-bd66df2c2b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m234.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install sentencepiece sacrebleu==2.4.2 --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0b239bc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b239bc8",
        "outputId": "4a870a83-167f-48a3-da3f-9fa25d2e7725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, math, time\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import sentencepiece as spm\n",
        "import sacrebleu\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "\n",
        "USE_DRIVE = True\n",
        "DATA_DIR = \"/content/drive/MyDrive/data\"\n",
        "EXPT_DIR = \"/content/drive/MyDrive/model\"\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(EXPT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33583ced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33583ced",
        "outputId": "3dd86eb0-fb10-4475-876e-3d9d9acfca8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found data at /content/drive/MyDrive/data\n",
            "OK /content/drive/MyDrive/data/train.de -> 160239 lines\n",
            "OK /content/drive/MyDrive/data/train.en -> 160239 lines\n",
            "OK /content/drive/MyDrive/data/valid.de -> 7283 lines\n",
            "OK /content/drive/MyDrive/data/valid.en -> 7283 lines\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def file_exists_set(data_dir):\n",
        "    req = [\"train.de\",\"train.en\",\"valid.de\",\"valid.en\"]\n",
        "    have = {f: Path(data_dir, f).exists() for f in req}\n",
        "    return have, all(have.values())\n",
        "\n",
        "have_map, all_ok = file_exists_set(DATA_DIR)\n",
        "\n",
        "if not all_ok:\n",
        "    demo_train = [\n",
        "        (\"hallo welt .\", \"hello world .\"),\n",
        "        (\"ich liebe maschinelles lernen .\", \"i love machine learning .\"),\n",
        "        (\"dies ist ein einfacher satz .\", \"this is a simple sentence .\"),\n",
        "        (\"wie geht es dir ?\", \"how are you ?\"),\n",
        "        (\"das ist ein buch .\", \"this is a book .\"),\n",
        "        (\"das wetter ist heute schoen .\", \"the weather is nice today .\"),\n",
        "        (\"ich habe hunger .\", \"i am hungry .\"),\n",
        "        (\"ich trinke kaffee .\", \"i drink coffee .\"),\n",
        "        (\"wo ist der bahnhof ?\", \"where is the train station ?\"),\n",
        "        (\"ich lerne deutsch .\", \"i am learning german .\"),\n",
        "        (\"ich spiele fussball gern .\", \"i like playing football .\"),\n",
        "        (\"er geht zur schule .\", \"he goes to school .\"),\n",
        "        (\"sie liest eine zeitung .\", \"she reads a newspaper .\"),\n",
        "        (\"wir reisen morgen .\", \"we travel tomorrow .\"),\n",
        "        (\"bitte sprechen sie langsam .\", \"please speak slowly .\"),\n",
        "        (\"kannst du mir helfen ?\", \"can you help me ?\"),\n",
        "        (\"ich verstehe nicht .\", \"i do not understand .\"),\n",
        "        (\"wo ist das badezimmer ?\", \"where is the bathroom ?\"),\n",
        "        (\"was kostet das ?\", \"how much is that ?\"),\n",
        "        (\"danke schoen .\", \"thank you very much .\"),\n",
        "    ]\n",
        "    demo_valid = [\n",
        "        (\"guten morgen .\", \"good morning .\"),\n",
        "        (\"gute nacht .\", \"good night .\"),\n",
        "        (\"ich komme aus deutschland .\", \"i come from germany .\"),\n",
        "        (\"er trinkt wasser .\", \"he drinks water .\"),\n",
        "        (\"das ist lecker .\", \"that is tasty .\"),\n",
        "        (\"bis spaeter !\", \"see you later !\"),\n",
        "    ]\n",
        "    with open(Path(DATA_DIR, \"train.de\"), \"w\", encoding=\"utf-8\") as f1,          open(Path(DATA_DIR, \"train.en\"), \"w\", encoding=\"utf-8\") as f2:\n",
        "        for de,en in demo_train:\n",
        "            f1.write(de+\"\\n\"); f2.write(en+\"\\n\")\n",
        "    with open(Path(DATA_DIR, \"valid.de\"), \"w\", encoding=\"utf-8\") as f1,          open(Path(DATA_DIR, \"valid.en\"), \"w\", encoding=\"utf-8\") as f2:\n",
        "        for de,en in demo_valid:\n",
        "            f1.write(de+\"\\n\"); f2.write(en+\"\\n\")\n",
        "    print(\"Created demo data at\", DATA_DIR)\n",
        "else:\n",
        "    print(\"Found data at\", DATA_DIR)\n",
        "\n",
        "for f in [\"train.de\",\"train.en\",\"valid.de\",\"valid.en\"]:\n",
        "    p = Path(DATA_DIR, f)\n",
        "    print((\"OK\" if p.exists() else \"MISS\"), p, (\"-> %d lines\" % sum(1 for _ in open(p, encoding=\"utf-8\"))) if p.exists() else \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3a3fa5b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3fa5b8",
        "outputId": "94d2155b-b2b3-4367-9f0f-4d1dbdc56cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs: 160239 | Valid pairs: 7283\n",
            "Example: ('und was menschliche gesundheit ist , kann auch ziemlich kompliziert sein .', 'and it can be a very complicated thing , what human health is .')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_parallel(src_path, tgt_path):\n",
        "    src_lines = Path(src_path).read_text(encoding=\"utf-8\").splitlines()\n",
        "    tgt_lines = Path(tgt_path).read_text(encoding=\"utf-8\").splitlines()\n",
        "    pairs = []\n",
        "    for s, t in zip(src_lines, tgt_lines):\n",
        "        s, t = s.strip(), t.strip()\n",
        "        if s and t:\n",
        "            pairs.append((s,t))\n",
        "    return pairs\n",
        "\n",
        "train_pairs = load_parallel(Path(DATA_DIR, \"train.de\"), Path(DATA_DIR, \"train.en\"))\n",
        "valid_pairs = load_parallel(Path(DATA_DIR, \"valid.de\"), Path(DATA_DIR, \"valid.en\"))\n",
        "print(\"Train pairs:\", len(train_pairs), \"| Valid pairs:\", len(valid_pairs))\n",
        "print(\"Example:\", train_pairs[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45fdf3f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45fdf3f5",
        "outputId": "b83d3311-738d-4a7c-d711-4c2a338b3bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reuse SentencePiece model\n",
            "Vocab size: 8000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SP_MODEL_PREFIX = Path(EXPT_DIR, \"spm_de_en\")\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "joint_corpus = Path(EXPT_DIR, \"joint_corpus.txt\")\n",
        "with open(joint_corpus, \"w\", encoding=\"utf-8\") as f:\n",
        "    for s,t in train_pairs:\n",
        "        f.write(s+\"\\n\"); f.write(t+\"\\n\")\n",
        "    for s,t in valid_pairs:\n",
        "        f.write(s+\"\\n\"); f.write(t+\"\\n\")\n",
        "\n",
        "if not Path(str(SP_MODEL_PREFIX)+\".model\").exists():\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=str(joint_corpus),\n",
        "        model_prefix=str(SP_MODEL_PREFIX),\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        model_type='bpe',\n",
        "        character_coverage=1.0,\n",
        "        pad_id=0, pad_piece=\"<pad>\",\n",
        "        unk_id=1, unk_piece=\"<unk>\",\n",
        "        bos_id=2, bos_piece=\"<s>\",\n",
        "        eos_id=3, eos_piece=\"</s>\"\n",
        "    )\n",
        "    print(\"Trained SentencePiece\")\n",
        "else:\n",
        "    print(\"Reuse SentencePiece model\")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(str(SP_MODEL_PREFIX)+\".model\")\n",
        "PAD_ID = sp.pad_id(); UNK_ID = sp.unk_id(); BOS_ID = sp.bos_id(); EOS_ID = sp.eos_id()\n",
        "print(\"Vocab size:\", len(sp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a3ae8871",
      "metadata": {
        "id": "a3ae8871"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_LEN = 64\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def encode_sentence(text: str):\n",
        "    ids = sp.encode(text, out_type=int)\n",
        "    ids = [BOS_ID] + ids + [EOS_ID]\n",
        "    return ids[:MAX_LEN]\n",
        "\n",
        "class ParallelDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        de, en = self.pairs[idx]\n",
        "        return torch.tensor(encode_sentence(de)), torch.tensor(encode_sentence(en))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    max_s = max(len(x) for x in src); max_t = max(len(y) for y in tgt)\n",
        "    def pad(list_tensors, L):\n",
        "        out = torch.full((len(list_tensors), L), PAD_ID, dtype=torch.long)\n",
        "        for i,t in enumerate(list_tensors):\n",
        "            out[i,:len(t)] = t\n",
        "        return out\n",
        "    return pad(src, max_s), pad(tgt, max_t)\n",
        "\n",
        "train_loader = DataLoader(ParallelDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(ParallelDataset(valid_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "10baa7d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10baa7d5",
        "outputId": "f872b1d3-d9ec-4dea-d5e8-f797b8cd47b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Params (M): 4.006\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div)\n",
        "        pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:,:x.size(1)]\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab, d_model=128, nhead=8, nlayers=2, ffn=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_src = nn.Embedding(vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.emb_tgt = nn.Embedding(vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.tr = nn.Transformer(d_model=d_model, nhead=nhead,\n",
        "                                 num_encoder_layers=nlayers, num_decoder_layers=nlayers,\n",
        "                                 dim_feedforward=ffn, dropout=dropout, batch_first=True)\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "    def forward(self, src, tgt_in):\n",
        "        src_mask = (src==PAD_ID)\n",
        "        tgt_mask = (tgt_in==PAD_ID)\n",
        "        T = tgt_in.size(1)\n",
        "        causal = torch.triu(torch.ones(T,T,dtype=torch.bool,device=tgt_in.device),1)\n",
        "        src = self.pos(self.emb_src(src))\n",
        "        tgt = self.pos(self.emb_tgt(tgt_in))\n",
        "        mem = self.tr.encoder(src, src_key_padding_mask=src_mask)\n",
        "        out = self.tr.decoder(tgt, mem, tgt_mask=causal,\n",
        "                              tgt_key_padding_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
        "        return self.proj(out)\n",
        "\n",
        "model = TransformerSeq2Seq(vocab=len(sp)).to(DEVICE)\n",
        "print(\"Params (M):\", round(sum(p.numel() for p in model.parameters())/1e6, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b742edfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b742edfb",
        "outputId": "26b34548-bd3b-4c1a-ef58-56bdfedfa593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch.compile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-787653911.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "W1023 08:17:44.702000 1349 torch/_inductor/utils.py:1436] [0/0_1] Not enough SMs to use max_autotune_gemm mode\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | trainCE 4.5947 | validCE 4.3200 | best 4.3200 | 229.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-787653911.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | trainCE 4.3315 | validCE 4.1864 | best 4.1864 | 142.2s\n",
            "Epoch 03 | trainCE 4.2516 | validCE 4.1077 | best 4.1077 | 141.6s\n",
            "Epoch 04 | trainCE 4.1789 | validCE 4.0446 | best 4.0446 | 140.7s\n",
            "\n",
            "Demo translations:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-787653911.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-787653911.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] DE: es ist diese pyramide .\n",
            "    EN: it &apos;s this great big .\n",
            "[1] DE: durch die muttermilch .\n",
            "    EN: through mother .\n",
            "[2] DE: es enthielt das zwei- , drei- , bis 400-fache des grenzwerts an schadstoffen der laut epa erlaubt war .\n",
            "    EN: it &apos;s the two-year-old , three-old-old-old-old-old-old-old-old-old-old dumbai .\n",
            "[3] DE: oft ist es abwasser , was uns verstopft .\n",
            "    EN: often , it &apos;s often what &apos;s called us .\n",
            "[4] DE: was macht man , wenn man solch eine unterbrechung im fluss hat ?\n",
            "    EN: what do you do , if you have a clipe in the middle of the right ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU on validation subset: BLEU = 15.97 45.9/19.9/10.8/6.6 (BP = 1.000 ratio = 1.012 hyp_len = 11017 ref_len = 10890)\n",
            "Checkpoint saved to: /content/drive/MyDrive/model/best.pt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "LR = 1e-3\n",
        "EPOCHS = 4\n",
        "LABEL_SMOOTH = 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=LABEL_SMOOTH)\n",
        "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), lr=LR)\n",
        "\n",
        "\n",
        "if torch.__version__.startswith(\"2\"):\n",
        "    try:\n",
        "        model = torch.compile(model)\n",
        "        print(\"Using torch.compile\")\n",
        "    except Exception as e:\n",
        "        print(\"torch.compile not available:\", e)\n",
        "\n",
        "\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    tot, tok = 0.0, 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(DEVICE, non_blocking=True), tgt.to(DEVICE, non_blocking=True)\n",
        "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        if train:\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            if use_amp:\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optim.step()\n",
        "\n",
        "        ntok = (tgt_out != PAD_ID).sum().item()\n",
        "        tot += loss.item() * ntok\n",
        "        tok += ntok\n",
        "    return tot / max(tok, 1)\n",
        "\n",
        "best = float(\"inf\")\n",
        "ckpt = Path(EXPT_DIR, \"best.pt\")\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    tr = run_epoch(train_loader, True)\n",
        "    va = run_epoch(valid_loader, False)\n",
        "    if va < best:\n",
        "        best = va\n",
        "        torch.save({\"model\": model.state_dict(),\n",
        "                    \"spm\": str(Path(EXPT_DIR, \"spm_de_en.model\"))}, ckpt)\n",
        "    print(f\"Epoch {ep:02d} | trainCE {tr:.4f} | validCE {va:.4f} | best {best:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(src_text, max_len=64):\n",
        "    model.eval()\n",
        "    s = torch.tensor([[2] + sp.encode(src_text, out_type=int) + [3]], device=DEVICE)\n",
        "    ys = torch.tensor([[2]], device=DEVICE)\n",
        "    for _ in range(max_len):\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logits = model(s, ys)\n",
        "        nxt = logits[0, -1].argmax(-1).item()\n",
        "        ys = torch.cat([ys, torch.tensor([[nxt]], device=DEVICE)], dim=1)\n",
        "        if nxt == 3:\n",
        "            break\n",
        "    out = ys[0, 1:].tolist()\n",
        "    if 3 in out:\n",
        "        out = out[:out.index(3)]\n",
        "    return sp.decode(out)\n",
        "\n",
        "\n",
        "state = torch.load(ckpt, map_location=DEVICE)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "\n",
        "\n",
        "n_show = min(5, len(valid_pairs))\n",
        "print(\"\\nDemo translations:\")\n",
        "for i in range(n_show):\n",
        "    src_txt = valid_pairs[i][0]\n",
        "    hyp = greedy_decode(src_txt)\n",
        "    print(f\"[{i}] DE: {src_txt}\")\n",
        "    print(f\"    EN: {hyp}\")\n",
        "\n",
        "\n",
        "MAX_EVAL = min(500, len(valid_pairs))\n",
        "refs = [[en for _, en in valid_pairs[:MAX_EVAL]]]\n",
        "hyps = [greedy_decode(de) for de, _ in valid_pairs[:MAX_EVAL]]\n",
        "print(\"\\nBLEU on validation subset:\", sacrebleu.corpus_bleu(hyps, refs).format())\n",
        "print(\"Checkpoint saved to:\", ckpt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}